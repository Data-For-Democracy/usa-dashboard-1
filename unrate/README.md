# usa-dashboard/urate

This project is for developing a calibrated time series "nowcast" for the
national unemployment rate.

The Bureau of Labor Statistics (BLS) releases a monthly estimate of the national
unemployment rate. We would like to be able to "interpolate" higher-frequency
weekly figures, but this requires additional data.  Pavlicek and Kristoufek
([pdf](https://arxiv.org/pdf/1408.6639.pdf)) show that Google Trends can
substantially improve the nowcast of the unemployment rate.

This project [`data`](./data/) directory currently has two unemployment series,
[`LNS14000000.csv`](./data/LNS14000000.csv') and
[`UNRATE.csv`](./data/UNRATE.csv'), which come from the BLS and Federal Reserve.
The Fed is a bit easier to get started with, but ultimately we want the data to
be coming from the BLS. The [`simple_model.py`](./scripts/simple_model.py)
program in the [`scripts`](./scripts/) directory takes the BLS series and
estimates an ARIMA model. Currently, it saves the results in a plot in
[`figures`](./figures/), depending on how the model is calibrated.

![ur-arima(4,1,0).svg](figures/ur-arima\(4,1,0\).svg)

# TODOs

## Pressing 

* Google search trends
    * Come up with a way to reliably pull data from Google search trends to be
      included with the model.
    * Decide which search terms the model should include (e.g. 'jobs', 'job
      listings', etc.).
    * Since we could potentially update the forecast every week, we need to
      decide how to interpolate the monthly BLS data with the new information
being generated by searches each week.
* Model verification and analysis
    * We should set up some diagnostics that our model can run through to check
      for various fitting problems.
    * This information should get logged somewhere so we can evaluate model
      performance going forward.
* Data storage
    * The CSV system is fine right now but eventually we will want to connect to
      the dashboard's database for storage.
    * Agreeing on data formatting and such will need to happen eventually.

## Less pressing

* State and city-level forecasting
    * In principle we could build identical models for individual states and
      possibly cities, although I'm not positive that the BLS has city
information.
    * We could be more specific in the search terms, e.g. 'Illinois jobs', that
      we calibrate on.
* BLS API wrapper
    * [`scripts/bls_query_to_csv.py`](./scripts/bls_query_to_csv.py) is a handy
      script that fetches time series data from the BLS and writes to a CSV file. 
    * This tool might be useful for other folks in D4D, so it would be cool to
      formalize this script as a fully-functional API wrapper. I don't think it
would be too difficult.

