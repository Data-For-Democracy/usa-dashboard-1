# usa-dashboard/urate

This project is for developing a calibrated time series "nowcast" for the
national unemployment rate.

The Bureau of Labor Statistics (BLS) releases a monthly estimate of the national
unemployment rate. We would like to be able to "interpolate" higher-frequency
weekly figures, but this requires additional data.  Pavlicek and Kristoufek
([pdf](https://arxiv.org/pdf/1408.6639.pdf)) show that Google Trends can
substantially improve the nowcast of the unemployment rate.

This project [`data`](./data/) directory houses BLS data in the [`national`](
./data/national) subdirectory, as well as Google search trends data in the 
[`google`](./data/google) directory.  The [`simple_model.py`](
./scripts/simple_model.py) program in the [`scripts`](./scripts/) directory 
takes the BLS series andestimates an ARIMA model. Currently, it saves the 
results in a plot in [`figures`](./figures/), depending on how the model is 
calibrated

[Here's an example plot.](./figures/ur-arima\(4,1,0\).svg)

# TODOs

## Pressing 

* Google search trends
    * ~~Come up with a way to reliably pull data from Google search trends 
    to be included with the model.~~ The `pytrends` library is really 
    useful, and we're using it to pull data from Google.
    * ~~Decide which search terms the model should include (e.g. 'jobs', 'job
      listings', etc.).~~ We're going to use 'jobs' as the first iteration.
    * Since we could potentially update the forecast every week, we need to
      decide how to interpolate the monthly BLS data with the new information
being generated by searches each week.
* Model verification and analysis
    * We should set up some diagnostics that our model can run through to check
      for various fitting problems.
    * This information should get logged somewhere so we can evaluate model
      performance going forward.
* Data storage
    * The CSV system is fine right now but eventually we will want to connect to
      the dashboard's database for storage.
    * Agreeing on data formatting and such will need to happen eventually.

## Less pressing

* State and city-level forecasting
    * In principle we could build identical models for individual states and
      possibly cities, although I'm not positive that the BLS has city
information.
    * We could be more specific in the search terms, e.g. 'Illinois jobs', that
      we calibrate on.
<s>
* BLS API wrapper
    * [`scripts/bls_query_to_csv.py`](./scripts/bls_query_to_csv.py) is a handy
      script that fetches time series data from the BLS and writes to a CSV file. 
    * This tool might be useful for other folks in D4D, so it would be cool to
      formalize this script as a fully-functional API wrapper. I don't think it
would be too difficult.
</s>
* We're using the [`bls`](https://github.com/OliverSherouse/bls) package to 
make requests from the BLS API.